\documentclass{article}

\usepackage{scribe}

\setseriestitle{Statistical and Algorithmic Learning Theory}
\setscribecode{4}
\setauthname{Gurpreet Singh}
\setinstrname{Purushottam Kar}
\setheaddate{January 17, 2018}
\settitle{PAC Learning and Agnostic Setting}

\begin{document}
\makeheader%

\begin{ssection}{PAC Learning}

	In the previous scribe, we have shown that for a finite hypothesis space, the ERM model for a sample with a sufficiently large size, independent of the distribution will be \et{probably approximately correct}. We can now formally define Probably Approximately Correct (PAC) model.

	\begin{definition}[PAC Learnability, \cite{ml-tta}]
		A hypothesis class $\cH$ is PAC learnable if there exist a function $m_\cH : (0, 1)^2 \ra \bN$ and a learning algorithm with the following property: For every $\epsilon, \delta \in (0, 1)$, for every distribution $\cD$ over $\cX$, and for every prediction function $f : \cX \ra \set{0, 1}$, if the realizable assumption holds with respect to $\cH$, $\cD$, $f$, then when running the learning algorithm on $m \ge m_\cH (\epsilon, \delta)$ i.i.d. examples generated by $\cD$ and labeled by $f$, the algorithm returns a hypothesis $\hat{f_S}$ for a training sample $S$ and loss function $l$ such that, with probability of at least $1 - \delta$ (over the choice of the examples), $L_{\cD, f}(\hat{f}) \le \epsilon$.
	\end{definition}

	In the above definition, $\epsilon$ is defined as the accuracy parameter, which allows a margin for error in prediction. This is attributed to the fact that $S$ is a finite sample, and there is a chance that it does not very faithfully represent the real distribution. $\delta$ is known as the confidence paramter. \br

	$m_\cH$ is the \et{sampling complexity} of the hypothesis class $\cH$ \et{i.e.} the number of minimum samples required to guarantee a probably approximately correct algorithm. From the previous scribe, we know that if
	\begin{equation}
		m \ge \frac{\log{\abs{\cH} / \delta}}{\epsilon}
	\end{equation}

	then we can have a PAC solution for the $\cH$. Therefore, the minimum sampling complexity must be less than this value. Hence, we can write
	\begin{equation}
		m_\cH	\le	\ceil{\frac{\log{\abs{\cH} / \delta}}{\epsilon}}
		\label{eq:sampling-complexity}
	\end{equation}

\end{ssection}

\begin{ssection}{Agnostic Setting --- Releasing the Realizability assumption}

	For now, we have assumed that the distribution $\cD$ is realizable \et{i.e.} $\qexists f \in \cF$ such that $\err[_D^l]{f} = 0$ for some loss function. However this is not a realistic setting, considering there can be noise in the observed data points $\cX, \cY$. It might be unwise to assume that the labels are completely determined by the features. \br

	For example, consider a binary classification problem, where for two feature vectors $\vx^1$ and $\vx^2$ have the same values of the features however $y^1 \ne y^2$. In this case, it is never possible to find a prediction function that gives zero error on the distribution. \br

	Therefore, we must relax the realizability assumption. In this case, we redefine the distribution $\cD$ to be a joint probability distribution over the feature space and the output space. \br

	Our goal is the same. We wish to find a prediction function $f^\ast$ that minimizes the l-risk. It should be clear that since we generally do not know the distribution $\cD$, we cannot find the optimal prediction function $f^\ast$, however we can only find a predictor that is probably approximately close to $f^\ast$. Hence, we define PAC learning, however, now for agnostic setting. \br

	\begin{definition}[Agnostic PAC Learnability, \cite{ml-tta}]
		A hypothesis class $\cH$ is agnostic PAC learnable if there exist a function $m_\cH : (0, 1)^2 \ra \bN$ and a learning algorithm with the following property: For every $\epsilon, \delta \in (0, 1)$ and for every distribution $\cD$ over $\cX \times \cY$, when running the learning algorithm on $m \ge m_\cH (\epsilon, \delta)$ i.i.d. examples generated by $\cD$, the algorithm returns a hypothesis $f_S$ for a training sample $S$ and a loss function $l$ such that, with probability of at least $1 - \delta$, (over the choice of the $m$ training examples),
		\begin{align*}
			\err[_S^l]{f_S}	\qle	\min_{f' \in \cH} \err[_D^l]{f'} + \epsilon
		\end{align*}
	\end{definition}

	\note{The definition of Agnostic PAC Learnability boils down to the PAC Learning under Realizable Assumption if the Assumption indeed holds true, as $\min_{f \in \cH} \err[_D^l]{f} = 0$}

	\begin{remark}
		We assume the loss function to be measurable for all functions and at all points \et{i.e.} $\qforall (x, y) \in \cX \times \cY$, $\qforall f \in \cH$, $l(f(x), y)$ is measurable. Note that if the loss function is not measurable, then it cannot be a random variable.
	\end{remark}
\end{ssection}

\begin{thebibliography}{10}

	\bibitem{ml-tta}
		Shai Shalev-Shwartz, Shai Ben-David
		\et{Understanding Machine Learning from Theory to Algorithms}. \\
		\href{http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning}{http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning}

\end{thebibliography}

\end{document}
